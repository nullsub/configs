\chapter{Netzwerkdateisysteme}
\section{NFS}
Mit einem NFS (Network File System) können mehrere Clienten auf Dateien
in einem Netzwerk zugreifen und diese so behandeln, als wären sie
auf der eigenen Festplatte.
Dies hat hier vorallem den Vorteil, dass man das home-Verzeichnis
exportieren und somit auf jedem Knoten einem User sein User-Verzeichnis
bereitstellen kann. Ausserdem wird ein shared-Verzeichnis exportiert, in dem 
alle Programme installiert werden, die von allen Knoten aus benutzt werden.
\begin{lstlisting}[style=Bash]
# apt-get install nfs-kernel-server nfs-common
\end{lstlisting}
In /etc/exports muss das Verzeichnis freigegeben werden:
\begin{lstlisting}[style=Bash,basicstyle=\small]
/home    192.168.2.0/255.255.255.0(rw,async,no_subtree_check,no_root_squash)
/shared  192.168.2.0/255.255.255.0(rw,async,no_subtree_check)
\end{lstlisting}
Die Computenodes mounten das nfs mit folgendem /etc/fstab Eintrag:
\begin{lstlisting}[style=Bash]
192.168.2.1:/home /home nfs rw 0 0
192.168.2.1:/shared /shared nfs rw 0 0
\end{lstlisting}
Jedem Nutzer wird der Zufgriff auf das Verzeichnis gewährt:
\begin{lstlisting}[style=Bash]
$ sudo chmod a+rwx
\end{lstlisting}
\section{Paralleles Verteiltes Dateisystem}
Bei einem parallelen verteiltem Dateisystem gibt es verschieden Anwendungsfälle.
Zum einen kann es als Verteiltes Dateisystem genutzt werden,
sodass die Speichergröße mit der Anzahl der Knoten skaliert.
Desweiteren kann es als Repliziertes Dateisystem benutzt werden,
um Ausfallsicherheit zu erhöhen.
Schließlich kann damit auch der Datendurchsatz erhäht werden, sowie eine Kombination aus allen 3 Varianten.
Für unser Cluster ist der möglichst große Datendurchsatz interessant.
Das parallele verteilte Dateisystem wurde mit GlusterFS realisiert,
da es einfach in der Installation ist und für unsere Zwecke völlig ausreicht.\\

Auf jedem Node wird nun glusterfs installiert und eine 20G Partition mit
XFS erstellt:
\begin{lstlisting}[style=Bash]
# apt-get install glusterfs-server
\end{lstlisting}
\begin{lstlisting}[style=Bash]
# fdisk /dev/sda
---------------
n <enter>
<enter>
<enter>
w
\end{lstlisting}
\begin{lstlisting}[style=Bash]
# apt-get install parted
# partprobe
\end{lstlisting}
\begin{lstlisting}[style=Bash]
# apt-get install xfsprogs
\end{lstlisting}
\begin{lstlisting}[style=Bash]
# mkfs.xfs -i size=512 /dev/sdX
# mkdir -p /data/bricki
\end{lstlisting}
/etc/fstab:
\begin{lstlisting}[style=Bash]
/dev/sdaX /data/brickN xfs defaults 1 2
\end{lstlisting}
\begin{lstlisting}[style=Bash]
mount -a && mount
\end{lstlisting}
Nun wird ein Striped-Volume(hoher Datendurchsatz) mit 2 Knoten erstellt
\begin{lstlisting}[style=Bash]
# gluster peer probe 192.168.2.10
# gluster volume create striped-volume stripe 2 
    192.168.2.1:/data/brick0 192.168.2.10:/data/brick1
# gluster volume start striped-volume
\end{lstlisting}
Auf jedem Node wird nun das Volume unter /fastfs gemountet:
\begin{lstlisting}[style=Bash]
# mkdir /fastfs 
%# mount.glusterfs 192.168.2.10:/striped-volumefastfs
\end{lstlisting}
/etc/fstab:
\begin{lstlisting}[style=Bash]
192.168.2.10:/striped-volume /fastfs glusterfs defaults,_netdev 0 0
\end{lstlisting}
Die Nachteile des Striped-Volume liegen auf der Hand. Fällt ein Knoten aus,
ist das ganze Volume unbrauchbar.\\
write speed /home: ~ 350MB/s\\
write speed /fastfs: ~ 40MB/s\\
write speed /shared: ~ 336MB/s\\
Obwohl das GlusterFS, welches unter /fastfs eingebunden wurde
die beste Performance liefern sollte, schneidet es als Schlechtestes ab.
